{"cells":[{"cell_type":"markdown","metadata":{},"source":["Load and preprocess the data"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-02T12:54:29.389060Z","iopub.status.busy":"2024-08-02T12:54:29.388608Z","iopub.status.idle":"2024-08-02T12:54:50.867973Z","shell.execute_reply":"2024-08-02T12:54:50.865357Z","shell.execute_reply.started":"2024-08-02T12:54:29.389025Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Warning: Unexpected format in word_tag: <Corpora\n","Warning: Unexpected format in word_tag: type=\"Monolingual-POS-TAGGED\"\n","Warning: Unexpected format in word_tag: Language=\"Hindi\">\n","Warning: Unexpected format in word_tag: </Corpora>\n","Number of sentences: 540\n","Example sentence: (['पूर्ण', 'प्रतिबंध', 'हटाओ', ':', 'इराक'], ['JJ', 'NN', 'VFM', 'SYM', 'NNP'])\n","Number of sentences: 540\n","Number of unique tags: 26\n","Tags: , NNPC, VRB, PUNC, VNN, NEG, SYM, VJJ, NNC, NVB, NN, INTF, VFM, QW, JJ, CC, VAUX, QF, JVB, QFNUM, NNP, RB, PREP, PRP, NLOC, RP\n","Training samples: 378\n","Validation samples: 81\n","Test samples: 81\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer\n","import torch\n","\n","# Function to parse the file\n","def parse_pos_file(file_path):\n","    sentences = []\n","    current_sentence = []\n","    \n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            line = line.strip()\n","            if line.startswith('<Sentence id='):\n","                if current_sentence:\n","                    sentences.append(current_sentence)\n","                current_sentence = []\n","            elif line and not line.startswith('</Sentence>'):\n","                current_sentence.extend(line.split())\n","        if current_sentence:\n","            sentences.append(current_sentence)\n","    \n","    data = []\n","    for sentence in sentences:\n","        words = []\n","        tags = []\n","        for word_tag in sentence:\n","            parts = word_tag.rsplit('_', 1)\n","            if len(parts) == 2:\n","                word, tag = parts\n","                words.append(word)\n","                tags.append(tag)\n","            else:\n","                print(f\"Warning: Unexpected format in word_tag: {word_tag}\")\n","        if words and tags:\n","            data.append((words, tags))\n","    \n","    return data\n","\n","# Load your dataset\n","file_path = 'indian\\hindi.pos'\n","data = parse_pos_file(file_path)\n","\n","# Print some information about the dataset\n","print(f\"Number of sentences: {len(data)}\")\n","if data:\n","    print(f\"Example sentence: {data[0]}\")\n","else:\n","    print(\"No valid sentences found in the data.\")\n","\n","# Split the data\n","train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n","val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n","\n","# Load the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# Function to encode the data\n","def encode_data(sentences, tokenizer, max_length=128):\n","    input_ids = []\n","    attention_masks = []\n","    labels = []\n","    \n","    for words, tags in sentences:\n","        encoded = tokenizer.encode_plus(\n","            words,\n","            is_split_into_words=True,\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True\n","        )\n","        \n","        input_ids.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","        \n","        # Adjust labels to match tokenized input\n","        label_ids = [-100] + [tag2id[tag] for tag in tags] + [-100] * (max_length - len(words) - 1)\n","        label_ids = label_ids[:max_length]\n","        labels.append(label_ids)\n","    \n","    return {\n","        'input_ids': torch.tensor(input_ids),\n","        'attention_mask': torch.tensor(attention_masks),\n","        'labels': torch.tensor(labels)\n","    }\n","\n","# Create tag to ID mapping\n","all_tags = set(tag for _, tags in data for tag in tags)\n","tag2id = {tag: id for id, tag in enumerate(all_tags)}\n","id2tag = {id: tag for tag, id in tag2id.items()}\n","\n","# Encode the datasets\n","train_encodings = encode_data(train_data, tokenizer)\n","val_encodings = encode_data(val_data, tokenizer)\n","test_encodings = encode_data(test_data, tokenizer)\n","\n","# Print some information about the dataset\n","print(f\"Number of sentences: {len(data)}\")\n","print(f\"Number of unique tags: {len(all_tags)}\")\n","print(f\"Tags: {', '.join(all_tags)}\")\n","print(f\"Training samples: {len(train_data)}\")\n","print(f\"Validation samples: {len(val_data)}\")\n","print(f\"Test samples: {len(test_data)}\")"]},{"cell_type":"markdown","metadata":{},"source":["Create dataset and dataloader"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.status.busy":"2024-08-02T12:50:54.288236Z","iopub.status.idle":"2024-08-02T12:50:54.288698Z","shell.execute_reply":"2024-08-02T12:50:54.288504Z","shell.execute_reply.started":"2024-08-02T12:50:54.288449Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class POSDataset(Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings['labels'])\n","\n","# Create datasets\n","train_dataset = POSDataset(train_encodings)\n","val_dataset = POSDataset(val_encodings)\n","test_dataset = POSDataset(test_encodings)\n","\n","# Create dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16)\n","test_loader = DataLoader(test_dataset, batch_size=16)"]},{"cell_type":"markdown","metadata":{},"source":["Set up the model"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.status.busy":"2024-08-02T12:50:54.289835Z","iopub.status.idle":"2024-08-02T12:50:54.290272Z","shell.execute_reply":"2024-08-02T12:50:54.290067Z","shell.execute_reply.started":"2024-08-02T12:50:54.290050Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import BertForTokenClassification\n","\n","num_labels = len(tag2id)\n","model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)"]},{"cell_type":"markdown","metadata":{},"source":["Set up training"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.status.busy":"2024-08-02T12:50:54.291885Z","iopub.status.idle":"2024-08-02T12:50:54.292319Z","shell.execute_reply":"2024-08-02T12:50:54.292100Z","shell.execute_reply.started":"2024-08-02T12:50:54.292084Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","\n","num_epochs = 10\n","num_training_steps = num_epochs * len(train_loader)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"]},{"cell_type":"markdown","metadata":{},"source":["Training loop"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2024-08-02T12:50:54.294476Z","iopub.status.idle":"2024-08-02T12:50:54.294884Z","shell.execute_reply":"2024-08-02T12:50:54.294716Z","shell.execute_reply.started":"2024-08-02T12:50:54.294701Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/24 [00:00<?, ?it/s]c:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n","100%|██████████| 24/24 [00:21<00:00,  1.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss: 2.7226, Val Loss: 2.6157\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 10/24 [00:09<00:13,  1.07it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[5], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 32\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 15\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n","File \u001b[1;32mc:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:647\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[0;32m    646\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m--> 647\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    650\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tqdm import tqdm\n","\n","def train(model, dataloader, optimizer, scheduler, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(dataloader):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","    return total_loss / len(dataloader)\n","\n","def evaluate(model, dataloader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            total_loss += outputs.loss.item()\n","    return total_loss / len(dataloader)\n","\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, optimizer, scheduler, device)\n","    val_loss = evaluate(model, val_loader, device)\n","    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-08-02T12:50:54.297394Z","iopub.status.idle":"2024-08-02T12:50:54.297964Z","shell.execute_reply":"2024-08-02T12:50:54.297710Z","shell.execute_reply.started":"2024-08-02T12:50:54.297687Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","          CC       0.24      0.10      0.14        40\n","        INTF       0.00      0.00      0.00         3\n","          JJ       0.42      0.09      0.15        54\n","         JVB       0.00      0.00      0.00        13\n","         NEG       0.00      0.00      0.00         7\n","        NLOC       0.00      0.00      0.00         7\n","          NN       0.28      0.45      0.34       249\n","         NNC       0.25      0.09      0.13        55\n","         NNP       0.48      0.38      0.42       119\n","        NNPC       0.12      0.09      0.11        32\n","         NVB       0.00      0.00      0.00        23\n","        PREP       0.33      0.49      0.40       291\n","         PRP       0.48      0.25      0.33        60\n","        PUNC       0.44      0.39      0.42        82\n","          QF       0.00      0.00      0.00         6\n","       QFNUM       0.37      0.47      0.41        75\n","          QW       0.00      0.00      0.00         1\n","          RB       0.67      0.13      0.22        15\n","          RP       0.00      0.00      0.00        20\n","         SYM       0.83      0.33      0.48        15\n","        VAUX       0.33      0.26      0.29        77\n","         VFM       0.15      0.15      0.15       103\n","         VJJ       0.00      0.00      0.00         6\n","         VNN       0.00      0.00      0.00        16\n","         VRB       0.00      0.00      0.00         5\n","\n","    accuracy                           0.32      1374\n","   macro avg       0.22      0.15      0.16      1374\n","weighted avg       0.31      0.32      0.30      1374\n","\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","c:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","c:\\Users\\91826\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["import torch\n","from sklearn.metrics import classification_report\n","\n","def get_predictions(model, dataloader, device):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            preds = torch.argmax(outputs.logits, dim=2)\n","            predictions.extend(preds.cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","    return predictions, true_labels\n","\n","# Get predictions\n","predictions, true_labels = get_predictions(model, test_loader, device)\n","\n","# Convert numeric labels back to text labels\n","label_map = id2tag  # We already created this mapping earlier\n","true_labels_text = [[label_map[l] for l in label if l != -100] for label in true_labels]\n","predictions_text = [[label_map[p] for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, true_labels)]\n","\n","# Flatten the lists\n","flattened_true_labels = [label for sublist in true_labels_text for label in sublist]\n","flattened_predictions = [label for sublist in predictions_text for label in sublist]\n","\n","# Print the classification report\n","print(classification_report(flattened_true_labels, flattened_predictions))\n"]},{"cell_type":"markdown","metadata":{},"source":["Save the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-08-02T12:50:54.299548Z","iopub.status.idle":"2024-08-02T12:50:54.300094Z","shell.execute_reply":"2024-08-02T12:50:54.299847Z","shell.execute_reply.started":"2024-08-02T12:50:54.299824Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('./pos_tagger_model\\\\tokenizer_config.json',\n"," './pos_tagger_model\\\\special_tokens_map.json',\n"," './pos_tagger_model\\\\vocab.txt',\n"," './pos_tagger_model\\\\added_tokens.json')"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained('./pos_tagger_model')\n","tokenizer.save_pretrained('./pos_tagger_model')"]},{"cell_type":"markdown","metadata":{},"source":["Use the model for tagging:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-08-02T12:50:54.302089Z","iopub.status.idle":"2024-08-02T12:50:54.302626Z","shell.execute_reply":"2024-08-02T12:50:54.302388Z","shell.execute_reply.started":"2024-08-02T12:50:54.302365Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[('पूर्ण', 'NNC'), ('प्रतिबंध', 'NN'), ('हटाओ', 'PREP'), (':', 'PREP'), ('इराक', 'PREP')]\n"]}],"source":["def tag_text(text, model, tokenizer, device):\n","    model.eval()\n","    words = text.split()\n","    inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=True)\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    predictions = torch.argmax(outputs.logits, dim=2)\n","    tags = [id2tag[p.item()] for p in predictions[0][1:-1]]  # Ignore [CLS] and [SEP] tokens\n","    return list(zip(words, tags))\n","\n","# Example usage\n","text = \"पूर्ण प्रतिबंध हटाओ : इराक\"\n","tagged = tag_text(text, model, tokenizer, device)\n","print(tagged)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2078,"sourceId":3524,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
